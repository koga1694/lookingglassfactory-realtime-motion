{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884142e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_cv_attention_models\n",
      "  Downloading keras_cv_attention_models-1.2.19-py3-none-any.whl (450 kB)\n",
      "Requirement already satisfied: tensorflow-addons in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from keras_cv_attention_models) (0.16.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from keras_cv_attention_models) (2.8.0)\n",
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.5.2-py3-none-any.whl (4.2 MB)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (1.0.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (3.10.0.2)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (2.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (0.24.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (3.20.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (1.20.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (1.12.1)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (13.0.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (1.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (1.44.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (1.6.3)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (1.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (58.0.4)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (3.2.1)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow->keras_cv_attention_models) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from astunparse>=1.6.0->tensorflow->keras_cv_attention_models) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (2.26.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (2.0.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (2.6.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (3.3.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras_cv_attention_models) (3.2.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-addons->keras_cv_attention_models) (2.13.3)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.7.0-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tensorflow-datasets->keras_cv_attention_models) (4.62.3)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.56.0-py2.py3-none-any.whl (241 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\pnu\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tqdm->tensorflow-datasets->keras_cv_attention_models) (0.4.4)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=178f232c3a7c7595985994f409bb0b01e2427c2ff8f173102322c53afc5f30c9\n",
      "  Stored in directory: c:\\users\\pnu\\appdata\\local\\pip\\cache\\wheels\\e1\\e8\\83\\ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "Successfully built promise\n",
      "Installing collected packages: googleapis-common-protos, tensorflow-metadata, promise, dill, tensorflow-datasets, keras-cv-attention-models\n",
      "Successfully installed dill-0.3.4 googleapis-common-protos-1.56.0 keras-cv-attention-models-1.2.19 promise-2.3 tensorflow-datasets-4.5.2 tensorflow-metadata-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_cv_attention_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944058b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd82bd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# 모델 불러오기 \n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras_cv_attention_models import mobilevit\n",
    "\n",
    "mm = keras.models.load_model('first_model.h5')\n",
    "\n",
    "# 제너레이터 쓴 버전이랑 안쓴 버전 둘다 테스트 해주세요|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33933a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_1 (Rescaling)     (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " random_flip (RandomFlip)    (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " random_rotation (RandomRota  (None, 256, 256, 3)      0         \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " mobilevit_s (Functional)    (None, 7)                 4954375   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,954,375\n",
      "Trainable params: 4,942,119\n",
      "Non-trainable params: 12,256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  \n",
    "  layers.experimental.preprocessing.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "mm = tf.keras.Sequential([\n",
    "    layers.Input(shape=(256, 256, 3)),\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "  mm\n",
    "])\n",
    "mm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f880dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제너레이터 쓴 버전\n",
    "\n",
    "import shutil\n",
    "import pathlib\n",
    "from tensorflow.keras.utils import image_dataset_from_directory \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_path = r'C:\\workplace\\holo-project\\data_split\\train'\n",
    "val_path = r'C:\\workplace\\holo-project\\data_split\\val'\n",
    "size = (256,256)\n",
    "batch_size = 16\n",
    "num_epochs = 50\n",
    "input_shape = (256,256,3)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "                                   rescale=1./255,\n",
    "#                                    rotation_range=40,\n",
    "#                                    shear_range=0.5,\n",
    "#                                    zoom_range=0.2,\n",
    "                                   vertical_flip=True,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "train_ds = train_datagen.flow_from_directory(train_path,\n",
    "                                                  target_size=size,\n",
    "                                                  interpolation='bicubic',\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=batch_size)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "                                  rescale=1./255)\n",
    "val_ds = valid_datagen.flow_from_directory(val_path,\n",
    "                                                  target_size=size,\n",
    "                                                  \n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=batch_size)\n",
    "\n",
    "# show class indices\n",
    "print('****************')\n",
    "for cls, idx in train_ds.class_indices.items():\n",
    "    print('Class #{} = {}'.format(idx, cls))\n",
    "print('****************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8a26bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73776\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "\n",
    "data_dir = pathlib.Path(r'C:\\BS\\data')\n",
    "print(len(list(data_dir.glob(r'*\\*.jpg'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a492e220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73776\n",
      "Found 73776 files belonging to 7 classes.\n",
      "Using 59021 files for training.\n",
      "Found 73776 files belonging to 7 classes.\n",
      "Using 14755 files for validation.\n",
      "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 7), dtype=tf.float32, name=None))>\n",
      "3689\n"
     ]
    }
   ],
   "source": [
    "# 데이터 생으로 쓴 버전\n",
    "\n",
    "import shutil\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "\n",
    "data_dir = pathlib.Path(r'C:\\BS\\data')\n",
    "print(len(list(data_dir.glob(r'*\\*.jpg'))))\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "# num_epochs = 100\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split = 0.2,\n",
    "    subset = 'training',\n",
    "    seed = 97,\n",
    "    shuffle=True,\n",
    "    image_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    label_mode = 'categorical')\n",
    "    \n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split = 0.2,\n",
    "    subset = 'validation',\n",
    "    seed = 97,\n",
    "    shuffle=True,\n",
    "    image_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    label_mode = 'categorical')\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)\n",
    "print(train_ds)\n",
    "print(len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c4ace05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 1.2453 - accuracy: 0.6078\n",
      "Epoch 1: val_loss improved from inf to 1.07114, saving model to mobileVIT_01-1.07.h5\n",
      "3689/3689 [==============================] - 1530s 411ms/step - loss: 1.2453 - accuracy: 0.6078 - val_loss: 1.0711 - val_accuracy: 0.6924 - lr: 2.0000e-04\n",
      "Epoch 2/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 1.0650 - accuracy: 0.7003\n",
      "Epoch 2: val_loss improved from 1.07114 to 1.01171, saving model to mobileVIT_02-1.01.h5\n",
      "3689/3689 [==============================] - 1519s 412ms/step - loss: 1.0650 - accuracy: 0.7003 - val_loss: 1.0117 - val_accuracy: 0.7231 - lr: 2.0000e-04\n",
      "Epoch 3/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.9909 - accuracy: 0.7368\n",
      "Epoch 3: val_loss did not improve from 1.01171\n",
      "3689/3689 [==============================] - 1519s 412ms/step - loss: 0.9909 - accuracy: 0.7368 - val_loss: 1.0237 - val_accuracy: 0.7182 - lr: 2.0000e-04\n",
      "Epoch 4/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.9424 - accuracy: 0.7643\n",
      "Epoch 4: val_loss improved from 1.01171 to 0.94273, saving model to mobileVIT_04-0.94.h5\n",
      "3689/3689 [==============================] - 1521s 412ms/step - loss: 0.9424 - accuracy: 0.7643 - val_loss: 0.9427 - val_accuracy: 0.7637 - lr: 2.0000e-04\n",
      "Epoch 5/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.9020 - accuracy: 0.7827\n",
      "Epoch 5: val_loss did not improve from 0.94273\n",
      "3689/3689 [==============================] - 1520s 412ms/step - loss: 0.9020 - accuracy: 0.7827 - val_loss: 0.9795 - val_accuracy: 0.7479 - lr: 2.0000e-04\n",
      "Epoch 6/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.8694 - accuracy: 0.7981\n",
      "Epoch 6: val_loss improved from 0.94273 to 0.92966, saving model to mobileVIT_06-0.93.h5\n",
      "3689/3689 [==============================] - 1520s 412ms/step - loss: 0.8694 - accuracy: 0.7981 - val_loss: 0.9297 - val_accuracy: 0.7712 - lr: 2.0000e-04\n",
      "Epoch 7/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.8448 - accuracy: 0.8133\n",
      "Epoch 7: val_loss did not improve from 0.92966\n",
      "3689/3689 [==============================] - 1519s 412ms/step - loss: 0.8448 - accuracy: 0.8133 - val_loss: 0.9352 - val_accuracy: 0.7713 - lr: 2.0000e-04\n",
      "Epoch 8/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.8192 - accuracy: 0.8252\n",
      "Epoch 8: val_loss improved from 0.92966 to 0.91564, saving model to mobileVIT_08-0.92.h5\n",
      "3689/3689 [==============================] - 1520s 412ms/step - loss: 0.8192 - accuracy: 0.8252 - val_loss: 0.9156 - val_accuracy: 0.7819 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.7974 - accuracy: 0.8364\n",
      "Epoch 9: val_loss did not improve from 0.91564\n",
      "3689/3689 [==============================] - 1518s 411ms/step - loss: 0.7974 - accuracy: 0.8364 - val_loss: 0.9175 - val_accuracy: 0.7796 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.7809 - accuracy: 0.8430\n",
      "Epoch 10: val_loss improved from 0.91564 to 0.90465, saving model to mobileVIT_10-0.90.h5\n",
      "3689/3689 [==============================] - 1520s 412ms/step - loss: 0.7809 - accuracy: 0.8430 - val_loss: 0.9047 - val_accuracy: 0.7901 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.7597 - accuracy: 0.8526\n",
      "Epoch 11: val_loss improved from 0.90465 to 0.87935, saving model to mobileVIT_11-0.88.h5\n",
      "3689/3689 [==============================] - 1522s 412ms/step - loss: 0.7597 - accuracy: 0.8526 - val_loss: 0.8793 - val_accuracy: 0.7987 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.7460 - accuracy: 0.8597\n",
      "Epoch 12: val_loss did not improve from 0.87935\n",
      "3689/3689 [==============================] - 1520s 412ms/step - loss: 0.7460 - accuracy: 0.8597 - val_loss: 0.8833 - val_accuracy: 0.8007 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.7345 - accuracy: 0.8658\n",
      "Epoch 13: val_loss did not improve from 0.87935\n",
      "3689/3689 [==============================] - 1520s 412ms/step - loss: 0.7345 - accuracy: 0.8658 - val_loss: 0.8812 - val_accuracy: 0.7984 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.6724 - accuracy: 0.8942\n",
      "Epoch 14: val_loss improved from 0.87935 to 0.85749, saving model to mobileVIT_14-0.86.h5\n",
      "3689/3689 [==============================] - 1520s 412ms/step - loss: 0.6724 - accuracy: 0.8942 - val_loss: 0.8575 - val_accuracy: 0.8159 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.6519 - accuracy: 0.9048\n",
      "Epoch 15: val_loss did not improve from 0.85749\n",
      "3689/3689 [==============================] - 1520s 412ms/step - loss: 0.6519 - accuracy: 0.9048 - val_loss: 0.8583 - val_accuracy: 0.8211 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.6403 - accuracy: 0.9118\n",
      "Epoch 16: val_loss did not improve from 0.85749\n",
      "3689/3689 [==============================] - 1521s 412ms/step - loss: 0.6403 - accuracy: 0.9118 - val_loss: 0.9002 - val_accuracy: 0.8078 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.6078 - accuracy: 0.9278\n",
      "Epoch 17: val_loss did not improve from 0.85749\n",
      "3689/3689 [==============================] - 1521s 412ms/step - loss: 0.6078 - accuracy: 0.9278 - val_loss: 0.8628 - val_accuracy: 0.8239 - lr: 5.0000e-05\n",
      "Epoch 18/100\n",
      "3689/3689 [==============================] - ETA: 0s - loss: 0.5954 - accuracy: 0.9341\n",
      "Epoch 18: val_loss did not improve from 0.85749\n",
      "3689/3689 [==============================] - 1520s 412ms/step - loss: 0.5954 - accuracy: 0.9341 - val_loss: 0.8626 - val_accuracy: 0.8261 - lr: 5.0000e-05\n",
      "923/923 [==============================] - 102s 110ms/step - loss: 0.8626 - accuracy: 0.8261\n",
      "Validation accuracy: 82.61%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "\n",
    "label_smoothing_factor = 0.1\n",
    "epochs = 100\n",
    "num_classes = 7\n",
    "initial_learning_rate=0.02\n",
    "steps = np.round(len(train_ds)/ 32)\n",
    "\n",
    "\n",
    "# 처음에는 learning rate는 0.002 / 배치사이즈는 할 수 있는 만큼 크게 해서 돌려주세요\n",
    "\n",
    "\n",
    "learning_rate = 0.0002  # 논문에서는 0.002에서 시작  \n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "loss_fn =  keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing_factor)\n",
    "\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5, # 저는 0.1 or 0.5로 조정하면서 했습니다. 함수 설명 보시고 커스텀 해주세요\n",
    "    patience=2, # 이것도 학습되는거 보시고 적절하게 바꾸시면 될거같습니다./ train에서 과적합이 너무 많이 뜨면 1~2, 괜찮으면 좀 더 높게\n",
    "    min_lr = 1e-8 \n",
    "    )    \n",
    "\n",
    "\n",
    "\n",
    "def run_experiment(epochs=epochs):\n",
    "    \n",
    "    mm.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 4)\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = 'mobileVIT_{epoch:02d}-{val_loss:.2f}.h5',\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only = True, verbose = 1,\n",
    "    )\n",
    "\n",
    "    mm.fit(\n",
    "        train_ds,\n",
    "        validation_data = val_ds,\n",
    "        epochs=epochs,\n",
    "        batch_size = 16, # 배치사이즈 \n",
    "        callbacks = [early_stopping, model_checkpoint, reduce_lr],\n",
    "    )\n",
    "    \n",
    "    _, accuracy = mm.evaluate(val_ds)\n",
    "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    return mm\n",
    "\n",
    "\n",
    "model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49c4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19702970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79885bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기 \n",
    "import tensorflow as tf\n",
    "import keras \n",
    "from keras_cv_attention_models import mobilevit\n",
    "\n",
    "mm = keras.models.load_model('mobile_VIT.h5')\n",
    "\n",
    "# 제너레이터 쓴 버전이랑 안쓴 버전 둘다 테스트 해주세요|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c485c6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-05-24 03:03:56,333 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tf2onnx\\convert.py\", line 640, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tf2onnx\\convert.py\", line 235, in main\n",
      "    graph_def, inputs, outputs, initialized_tables, tensors_to_rename = tf_loader.from_saved_model(\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tf2onnx\\tf_loader.py\", line 614, in from_saved_model\n",
      "    _from_saved_model_v2(model_path, input_names, output_names,\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tf2onnx\\tf_loader.py\", line 552, in _from_saved_model_v2\n",
      "    imported = tf.saved_model.load(model_path, tags=tag)  # pylint: disable=no-value-for-parameter\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 782, in load\n",
      "    result = load_partial(export_dir, None, tags, options)[\"root\"]\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 887, in load_partial\n",
      "    loader_impl.parse_saved_model_with_debug_info(export_dir))\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 57, in parse_saved_model_with_debug_info\n",
      "    saved_model = parse_saved_model(export_dir)\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 115, in parse_saved_model\n",
      "    raise IOError(\n",
      "OSError: SavedModel file does not exist at: 'mobile_VIT.h5'\\{saved_model.pbtxt|saved_model.pb}\n"
     ]
    }
   ],
   "source": [
    "!python -m tf2onnx.convert --opset 13 --saved-model 'mobile_VIT.h5' --output 'mobile_VIT.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d023f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "2022-05-24 03:10:19,635 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tf2onnx\\convert.py\", line 640, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tf2onnx\\convert.py\", line 235, in main\n",
      "    graph_def, inputs, outputs, initialized_tables, tensors_to_rename = tf_loader.from_saved_model(\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tf2onnx\\tf_loader.py\", line 614, in from_saved_model\n",
      "    _from_saved_model_v2(model_path, input_names, output_names,\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tf2onnx\\tf_loader.py\", line 552, in _from_saved_model_v2\n",
      "    imported = tf.saved_model.load(model_path, tags=tag)  # pylint: disable=no-value-for-parameter\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 782, in load\n",
      "    result = load_partial(export_dir, None, tags, options)[\"root\"]\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 887, in load_partial\n",
      "    loader_impl.parse_saved_model_with_debug_info(export_dir))\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 57, in parse_saved_model_with_debug_info\n",
      "    saved_model = parse_saved_model(export_dir)\n",
      "  File \"C:\\Users\\pusan\\.conda\\envs\\gibo-project\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py\", line 115, in parse_saved_model\n",
      "    raise IOError(\n",
      "OSError: SavedModel file does not exist at: mobile_VIT.h5\\{saved_model.pbtxt|saved_model.pb}\n"
     ]
    }
   ],
   "source": [
    "!python -m tf2onnx.convert --saved-model mobile_VIT.h5 --opset 13 --output mobile_VIT.h5.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e18117f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 35). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\workplace\\zmq\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\workplace\\zmq\\assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "mm = keras.models.load_model('mobile_VIT.h5')\n",
    "\n",
    "\n",
    "\n",
    "mm.save('C:\\workplace\\zmq', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a259ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceecf1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb3664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
